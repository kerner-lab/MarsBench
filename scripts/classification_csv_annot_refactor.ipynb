{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from marsbench.data.classification.BaseClassificationDataset import BaseClassificationDataset\n",
    "from marsbench.utils.transforms import get_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='DeepMars_Landmark'\n",
    "from hydra import initialize_config_dir, compose\n",
    "import os\n",
    "config_dir = os.path.abspath('../configs')\n",
    "with initialize_config_dir(config_dir=config_dir, version_base='1.1'):\n",
    "    cfg = compose(config_name='config', overrides=[f\"data={data.lower()}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a annotation csv file\n",
    "txt_file = cfg.data.txt_file\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as text:\n",
    "    text = text.read().splitlines()\n",
    "df = pd.DataFrame([x.split() for x in text])\n",
    "df.columns = ['image_path', 'label']\n",
    "df['label'] = df['label'].replace('6', '5').astype(int)\n",
    "df.to_csv(cfg.data.annot_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union, Literal\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class DeepMars_Landmark(BaseClassificationDataset):\n",
    "    \"\"\"\n",
    "    DeepMars_Landmark https://zenodo.org/records/1048301\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        data_dir,\n",
    "        transform,\n",
    "        annot_csv: Union[str, os.PathLike],\n",
    "        split: Literal['train', 'val', 'test'] = 'train',\n",
    "        generator: Optional[torch.Generator] = None,\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.annot = pd.read_csv(annot_csv)\n",
    "        generator = torch.Generator().manual_seed(cfg.seed) if generator is None else generator\n",
    "        total_size = len(self.annot)\n",
    "        self.indices = self.determine_data_splits(total_size, generator, split)\n",
    "        super(DeepMars_Landmark, self).__init__(cfg, data_dir, transform)\n",
    "        \n",
    "    def _load_data(self) -> Tuple[List[str], List[int]]:\n",
    "        annot_subset = self.annot if self.indices is None else self.annot.iloc[self.indices]\n",
    "        image_paths = annot_subset['image_path'].astype(str).tolist()\n",
    "        labels = annot_subset['label'].astype(int).tolist()\n",
    "        return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, test_transform = get_transforms(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepMars_Landmark(cfg, cfg.data.data_dir, train_transform, cfg.data.annot_csv, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='DeepMars_Surface'\n",
    "from hydra import initialize_config_dir, compose\n",
    "import os\n",
    "config_dir = os.path.abspath('../configs')\n",
    "with initialize_config_dir(config_dir=config_dir, version_base='1.1'):\n",
    "    cfg = compose(config_name='config', overrides=[f\"data={data.lower()}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data.txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for split in cfg.data.txt_files:\n",
    "    with open(cfg.data.txt_files.get(split), \"r\", encoding=\"utf-8\") as text:\n",
    "        text = text.read().splitlines()\n",
    "    df_split = pd.DataFrame([x.split() for x in text])\n",
    "    df_split.columns = ['image_path', 'label']\n",
    "    df_split['split'] = split\n",
    "    df = pd.concat([df, df_split])\n",
    "df['label'] = df['label'].replace({'23':'22', '24':'23'}).astype(int)\n",
    "df.to_csv(cfg.data.annot_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMars_Surface(BaseClassificationDataset):\n",
    "    \"\"\"\n",
    "    DeepMars_Surface https://zenodo.org/records/1049137\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            cfg, \n",
    "            data_dir, \n",
    "            transform, \n",
    "            annot_csv: Union[str, os.PathLike],\n",
    "            split: Literal['train', 'val', 'test'] = 'train'):\n",
    "        self.annot = pd.read_csv(annot_csv)\n",
    "        self.split = split\n",
    "        super(DeepMars_Surface, self).__init__(cfg, data_dir, transform)\n",
    "\n",
    "    def _load_data(self) -> Tuple[List[str], List[int]]:\n",
    "        annot_subset = self.annot[self.annot['split'] == self.split]\n",
    "        image_paths = annot_subset['image_path'].astype(str).tolist()\n",
    "        labels = annot_subset['label'].astype(int).tolist()\n",
    "        return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, test_transform = get_transforms(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepMars_Surface(cfg, cfg.data.data_dir, train_transform, cfg.data.annot_csv, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='DoMars16k'\n",
    "from hydra import initialize_config_dir, compose\n",
    "import os\n",
    "config_dir = os.path.abspath('../configs')\n",
    "with initialize_config_dir(config_dir=config_dir, version_base='1.1'):\n",
    "    cfg = compose(config_name='config', overrides=[f\"data={data.lower()}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from itertools import chain\n",
    "df = pd.DataFrame()\n",
    "for split in cfg.data.data_dir:\n",
    "    data_dir = cfg.data.data_dir.get(split)\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    extensions = cfg.data.valid_image_extensions\n",
    "    for label, class_dir in enumerate(os.listdir(data_dir)):\n",
    "        class_dir_path = os.path.join(data_dir, class_dir)\n",
    "        matched_files = list(\n",
    "            chain.from_iterable(\n",
    "                glob.glob(os.path.join(class_dir_path, f\"*.{ext}\"))\n",
    "                for ext in extensions\n",
    "            )\n",
    "        )\n",
    "        image_paths.extend([os.path.relpath(file, data_dir) for file in matched_files])\n",
    "        labels.extend([label] * len(matched_files))\n",
    "    df_split = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
    "    df_split['split'] = split\n",
    "    df = pd.concat([df, df_split])\n",
    "\n",
    "df.to_csv(cfg.data.annot_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from typing import Tuple, Union, Literal\n",
    "import pandas as pd\n",
    "\n",
    "class DoMars16k(BaseClassificationDataset):\n",
    "    \"\"\"\n",
    "    DoMars16k dataset https://zenodo.org/records/4291940\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        data_dir,\n",
    "        transform,\n",
    "        annot_csv: Union[str, os.PathLike],\n",
    "        split: Literal[\"train\", \"val\", \"test\"] = \"train\",\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.annot = pd.read_csv(annot_csv)\n",
    "        self.annot = self.annot[self.annot[\"split\"] == split]\n",
    "        data_dir = data_dir + f\"/{split}\"\n",
    "        super(DoMars16k, self).__init__(cfg, data_dir, transform)\n",
    "\n",
    "    def _load_data(self) -> Tuple[List[str], List[int]]:\n",
    "        image_paths = self.annot['image_path'].astype(str).tolist()\n",
    "        labels = self.annot['label'].astype(int).tolist()\n",
    "        return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, test_transform = get_transforms(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DoMars16k(cfg, cfg.data.data_dir, train_transform, cfg.data.annot_csv, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='HiRISENet'\n",
    "from hydra import initialize_config_dir, compose\n",
    "import os\n",
    "config_dir = os.path.abspath('../configs')\n",
    "with initialize_config_dir(config_dir=config_dir, version_base='1.1'):\n",
    "    cfg = compose(config_name='config', overrides=[f\"data={data}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = cfg.data.txt_file\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as text:\n",
    "    rows = []\n",
    "    for line in text:\n",
    "        image_name, class_type_str, split_style = line.strip().split()[:3]\n",
    "        rows.append({'image_path': image_name, 'label': class_type_str, 'split': split_style})\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(cfg.data.annot_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiRISENet(BaseClassificationDataset):\n",
    "    \"\"\"\n",
    "    Mars Image Content Classfication-HiRISENet https://zenodo.org/records/4002935\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        data_dir,\n",
    "        transform,\n",
    "        annot_csv: Union[str, os.PathLike],\n",
    "        split: Literal[\"train\", \"val\", \"test\"] = \"train\",\n",
    "    ):\n",
    "        self.annot = pd.read_csv(annot_csv)\n",
    "        self.annot = self.annot[self.annot['split'] == split]\n",
    "        super(HiRISENet, self).__init__(cfg, data_dir, transform)\n",
    "\n",
    "    def _load_data(self) -> Tuple[List[str], List[int]]:\n",
    "        image_paths = self.annot['image_path'].astype(str).tolist()\n",
    "        labels = self.annot['label'].astype(int).tolist()\n",
    "        return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, test_transform = get_transforms(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HiRISENet(cfg, cfg.data.data_dir, train_transform, cfg.data.annot_csv, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='MartianFrost'\n",
    "from hydra import initialize_config_dir, compose\n",
    "import os\n",
    "config_dir = os.path.abspath('../configs')\n",
    "with initialize_config_dir(config_dir=config_dir, version_base='1.1'):\n",
    "    cfg = compose(config_name='config', overrides=[f\"data={data.lower()}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = cfg.data.data_dir\n",
    "df = pd.DataFrame()\n",
    "for split in cfg.data.txt_files:\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    txt_file = cfg.data.txt_files.get(split)\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as text:\n",
    "        valid_parents = set(line.strip() for line in text)\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    patterns = [(\"frost\", 1), (\"background\", 0)]\n",
    "\n",
    "    for subfolder, label in patterns:\n",
    "        for image_path in data_dir.glob(f\"*/tiles/{subfolder}/*\"):\n",
    "            each_folder = image_path.parents[2].name\n",
    "            parent_directory = each_folder[:15]\n",
    "            if parent_directory in valid_parents:\n",
    "                image_paths.append(str(image_path.relative_to(data_dir)))\n",
    "                labels.append(label)\n",
    "    df_split = pd.DataFrame({'image_path': image_paths, 'label': labels, 'split': split})\n",
    "    df = pd.concat([df, df_split])\n",
    "df.to_csv(cfg.data.annot_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MartianFrost(BaseClassificationDataset):\n",
    "    \"\"\"\n",
    "    Martian Frost dataset\n",
    "    https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi:10.48577/jpl.QJ9PYA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        data_dir,\n",
    "        transform,\n",
    "        annot_csv: Union[str, os.PathLike],\n",
    "        split: Literal[\"train\", \"val\", \"test\"] = \"train\",\n",
    "    ):\n",
    "        self.annot = pd.read_csv(annot_csv)\n",
    "        self.annot = self.annot[self.annot['split'] == split]\n",
    "        super(MartianFrost, self).__init__(cfg, data_dir, transform)\n",
    "\n",
    "    def _load_data(self) -> Tuple[List[str], List[int]]:\n",
    "        image_paths = self.annot['image_path'].astype(str).tolist()\n",
    "        labels = self.annot['label'].astype(int).tolist()\n",
    "        return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, test_transform = get_transforms(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MartianFrost(cfg, cfg.data.data_dir, train_transform, cfg.data.annot_csv, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='MSLNet'\n",
    "from hydra import initialize_config_dir, compose\n",
    "import os\n",
    "config_dir = os.path.abspath('../configs')\n",
    "with initialize_config_dir(config_dir=config_dir, version_base='1.1'):\n",
    "    cfg = compose(config_name='config', overrides=[f\"data={data}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for split in cfg.data.txt_files:\n",
    "    txt_file = cfg.data.txt_files.get(split)\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as text:\n",
    "        text = text.read().splitlines()\n",
    "    df_split = pd.DataFrame([x.split() for x in text])\n",
    "    df_split.columns = ['image_path', 'label']\n",
    "    df_split['split'] = split\n",
    "    df = pd.concat([df, df_split])\n",
    "df.to_csv(cfg.data.annot_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSLNet(BaseClassificationDataset):\n",
    "    \"\"\"\n",
    "    Mars Image Content Classification Mastcam & MAHILI Dataset\n",
    "    https://zenodo.org/records/4033453\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        data_dir,\n",
    "        transform,\n",
    "        annot_csv: Union[str, os.PathLike],\n",
    "        split: Literal[\"train\", \"val\", \"test\"] = \"train\",\n",
    "    ):\n",
    "        self.annot = pd.read_csv(annot_csv)\n",
    "        self.annot = self.annot[self.annot['split'] == split]\n",
    "        super(MSLNet, self).__init__(cfg, data_dir, transform)\n",
    "\n",
    "    def _load_data(self) -> Tuple[List[str], List[int]]:\n",
    "        image_paths = self.annot['image_path'].astype(str).tolist()\n",
    "        labels = self.annot['label'].astype(int).tolist()\n",
    "        return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, test_transform = get_transforms(cfg)\n",
    "train_dataset = MSLNet(cfg, cfg.data.data_dir, train_transform, cfg.data.annot_csv, split='train')\n",
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
