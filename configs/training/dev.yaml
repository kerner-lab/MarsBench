# Training specific parameters
batch_size: 32
num_workers: 4

# PyTorch Lightning Trainer arguments
trainer:
  max_epochs: 10
  accelerator: 'auto'      # auto-detects GPU/CPU
  devices: 'auto'          # auto-detects number of available devices
  strategy: 'auto'         # auto-selects DDP for multi-GPU
  sync_batchnorm: true    # important for multi-GPU training

  # Mixed precision settings
  precision: '16-mixed'    # Use mixed precision training

  enable_checkpointing: true

  # Optional: any additional trainer args that might be needed
  enable_progress_bar: true
  log_every_n_steps: 10

  # Multi-GPU specific settings
  num_nodes: 1            # number of compute nodes
  accumulate_grad_batches: 1  # gradient accumulation steps
